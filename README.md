# Churn Prediction Data Pipeline

This project is a lightweight but complete end-to-end data pipeline for customer churn prediction. It demonstrates real-world data engineering workflows using Apache Spark, scikit-learn, Amazon S3, and PostgreSQL.

## ğŸš€ Project Overview

The pipeline performs the following steps:

1. **Extract** raw customer data from Amazon S3  
2. **Transform** data using PySpark (ETL)  
3. **Train** a churn prediction model (e.g., logistic regression)  
4. **Predict** churn outcomes  
5. **Load** results into a PostgreSQL database  

---

## ğŸ› ï¸ Tech Stack

- **Apache Spark** (PySpark) â€“ for distributed data processing  
- **Amazon S3** â€“ cloud storage for raw and processed data  
- **scikit-learn** â€“ machine learning modeling  
- **PostgreSQL** â€“ storing prediction results   
- **Python 3.10+** â€“ primary programming language

---

## ğŸ“ Project Structure
churn_pipeline/  
â”‚  
â”œâ”€â”€ api/ # API endpoint to serve predictions (e.g., FastAPI or Flask)  
â”‚ â””â”€â”€ main.py  
â”‚  
â”œâ”€â”€ data/ # Placeholder for data files (raw, processed, etc.)  
â”‚  
â”œâ”€â”€ database/ # PostgreSQL integration  
â”‚ â”œâ”€â”€ db_config.py  
â”‚ â”œâ”€â”€ fill_db.py  
â”‚ â””â”€â”€ logs/  
â”‚  
â”œâ”€â”€ logs/ # Logs generated by scripts  
â”‚  
â”œâ”€â”€ model/ # Model training and artifacts  
â”‚ â”œâ”€â”€ train.py  
â”‚ â”œâ”€â”€ churn_model.pkl  
â”‚ â””â”€â”€ features.pkl  
â”‚  
â”œâ”€â”€ serve/ # (Optional) Serving logic for deployed model  
â”‚  
â”œâ”€â”€ spark_etl/ # ETL scripts using Apache Spark  
â”‚ â”œâ”€â”€ etl.py  
â”‚  
â”œâ”€â”€ .gitignore # Git ignored files (e.g., .env, .pkl)  
â”œâ”€â”€ paths_config.py  
â”œâ”€â”€ .env  
â”œâ”€â”€ main.py # Entry point (if any)  
â””â”€â”€ README.md # This file  
  
---

## ğŸ”§ Features

- âœ… PySpark-based ETL pipeline
- âœ… scikit-learn churn model training
- âœ… PostgreSQL storage for model output
- âœ… Airflow DAG for orchestration
- âœ… REST API endpoint to serve predictions
- âœ… `.env`-based secrets management (not tracked in Git)

---

## ğŸš€ Getting Started

### 1. Clone the Repository

git clone https://github.com/MarynaHryshko/churn_pipeline.git  
cd churn_pipeline  

### 2. Set Up Environment

python3 -m venv venv  
source venv/bin/activate  
pip install -r requirements.txt  

### 3. Configure .env
Create a .env file  

AWS_ACCESS_KEY_ID=...  
AWS_SECRET_ACCESS_KEY=...  
S3_BUCKET=your-bucket-name  
POSTGRES_HOST=...  
POSTGRES_DB=...  
POSTGRES_USER=...  
POSTGRES_PASSWORD=...  

## ğŸ§ª Run the Pipeline

### 1. Spark ETL
python spark_etl/etl.py

### 2. Model Training
python model/train.py

### 3. Load Predictions to PostgreSQL
python database/fill_db.py



