# Churn Prediction Data Pipeline

This project is a lightweight but complete end-to-end data pipeline for customer churn prediction. It demonstrates real-world data engineering workflows using Apache Spark, scikit-learn, Amazon S3, and PostgreSQL.

## 🚀 Project Overview

The pipeline performs the following steps:

1. **Extract** raw customer data from Amazon S3  
2. **Transform** data using PySpark (ETL)  
3. **Train** a churn prediction model (e.g., logistic regression)  
4. **Predict** churn outcomes  
5. **Load** results into a PostgreSQL database  

---

## 🛠️ Tech Stack

- **Apache Spark** (PySpark) – for distributed data processing  
- **Amazon S3** – cloud storage for raw and processed data  
- **scikit-learn** – machine learning modeling  
- **PostgreSQL** – storing prediction results   
- **Python 3.10+** – primary programming language

---

## 📁 Project Structure
churn_pipeline/  
│  
├── api/ # API endpoint to serve predictions (e.g., FastAPI or Flask)  
│ └── main.py  
│  
├── data/ # Placeholder for data files (raw, processed, etc.)  
│  
├── database/ # PostgreSQL integration  
│ ├── db_config.py  
│ ├── fill_db.py  
│ └── logs/  
│  
├── logs/ # Logs generated by scripts  
│  
├── model/ # Model training and artifacts  
│ ├── train.py  
│ ├── churn_model.pkl  
│ └── features.pkl  
│  
├── serve/ # (Optional) Serving logic for deployed model  
│  
├── spark_etl/ # ETL scripts using Apache Spark  
│ ├── etl.py  
│  
├── .gitignore # Git ignored files (e.g., .env, .pkl)  
├── paths_config.py  
├── .env  
├── main.py # Entry point (if any)  
└── README.md # This file  
  
---

## 🔧 Features

- ✅ PySpark-based ETL pipeline
- ✅ scikit-learn churn model training
- ✅ PostgreSQL storage for model output
- ✅ Airflow DAG for orchestration
- ✅ REST API endpoint to serve predictions
- ✅ `.env`-based secrets management (not tracked in Git)

---

## 🚀 Getting Started

### 1. Clone the Repository

git clone https://github.com/MarynaHryshko/churn_pipeline.git  
cd churn_pipeline  

### 2. Set Up Environment

python3 -m venv venv  
source venv/bin/activate  
pip install -r requirements.txt  

### 3. Configure .env
Create a .env file  

AWS_ACCESS_KEY_ID=...  
AWS_SECRET_ACCESS_KEY=...  
S3_BUCKET=your-bucket-name  
POSTGRES_HOST=...  
POSTGRES_DB=...  
POSTGRES_USER=...  
POSTGRES_PASSWORD=...  

## 🧪 Run the Pipeline

### 1. Spark ETL
python spark_etl/etl.py

### 2. Model Training
python model/train.py

### 3. Load Predictions to PostgreSQL
python database/fill_db.py



